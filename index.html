<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Haowei Li</title>

  <meta name="author" content="Haowei Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/ut_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Haowei Li</name>
              </p>
				<p>
				  I am currently pursuing a Ph.D. degree in Electrical Engineering at The Pennsylvania State University.
				</p>
				<p>
				  My current research focuses on the application of <b>large language models (LLMs) in extended reality (XR) systems</b>, including intelligent scene generation, interactive agents, and multimodal human–computer interaction.
				</p>
				<p>
				  I received both my bachelor’s and master’s degrees in Communications Engineering from Xidian University.
				  During my master’s studies, my research focused on <b>efficient and scalable training and inference of large language models</b>, with an emphasis on distributed learning systems.
				</p>

                <p style="text-align:center">
                <a href="mailto:hql5799@psu.edu">Email</a> &nbsp;/&nbsp;
                <a href="https://github.com/HaoweiLi778">Github</a> &nbsp;/&nbsp;
				<a href="https://scholar.google.com/citations?hl=zh-CN&user=3YGe2_0AAAAJ">Google Scholar</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="img/per.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="img/per.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        
        
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody>
    <tr>
      <td>
        <heading>EDUCATION</heading>

        <p>
          <b>The Pennsylvania State University,</b><i> University Park, PA, USA</i>
          <span style="float:right;">09/2025–Present</span>
        </p>
        <p>
        <ul>
          <li style="line-height:30px;"><b>Degree:</b> Ph.D. in Electrical Engineering</li>
		<li style="line-height:30px;"><b>Advisor:</b> Prof. Bin Li</li>
          <li style="line-height:30px;"><b>Research:</b> Large Language Models for Extended Reality (XR)</li>
        </ul>
        </p>

        <p>&nbsp;</p>

        <p>
          <b>Xidian University,</b><i> Xi’an, China</i>
          <span style="float:right;">09/2023–06/2025</span>
        </p>
        <p>
        <ul>
          <li style="line-height:30px;"><b>Degree:</b> M.E. in Communications Engineering</li>
          <li style="line-height:30px;"><b>School:</b> School of Telecommunications Engineering</li>
          <li style="line-height:30px;"><b>Advisor:</b> Prof. Weiying Xie</li>
          <li style="line-height:30px;"><b>Research:</b> Efficient distributed training and inference of large language models</li>
          <li style="line-height:30px;"><b>Outstanding Engineer Experimental Class</b></li>
        </ul>
        </p>

        <p>&nbsp;</p>

        <p>
          <b>Xidian University,</b><i> Xi’an, China</i>
          <span style="float:right;">09/2019–06/2023</span>
        </p>
        <p>
        <ul>
          <li style="line-height:30px;"><b>Degree:</b> B.E. in Communication Engineering</li>
          <li style="line-height:30px;"><b>School:</b> School of Telecommunications Engineering</li>
          <li style="line-height:30px;"><b>Outstanding Engineer Experimental Class</b></li>
        </ul>
        </p>

      </td>
    </tr>
  </tbody>
</table>






        
        
        
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody>
    <tr>
      <td>
        <heading>RESEARCH INTERESTS</heading>
        <p>
        <ul>
          <li style="line-height:30px;"><b>Distributed Machine Learning</b></li>
          <li style="line-height:30px;"><b>Large Language Models (LLMs)</b></li>
          <li style="line-height:30px;"><b>Extended Reality (XR)</b></li>
          <li style="line-height:30px;"><b>Efficient Training and Inference</b></li>
        </ul>
        </p>
      </td>
    </tr>
  </tbody>
</table>



        
        
        
        
        
        
        
        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>PUBLICATIONS</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="img/1.jpg"><img src="img/1.jpg" alt="Publications" width="280" height="160" style="border-style: none"></a>
            </td>
            <td width="75%" valign="middle">
              <b>JointSQ: Joint Sparsification-Quantization for Distributed Learning</b>
              <p>Author: Weiying Xie* (Advisor), Haowei Li*, Jitao Ma, Yunsong Li, Jie Lei, Donglai Liu and Leyuan Fang.</p>
              <ul>
              	<li>We construct a Sparsification-Quantization joint learning framework to compress communication in distributed machine learning. </li>
			    <li>Has been accepted in IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR-
2024).</li>
<li>
  <a href="https://github.com/HaoweiLi778/JointSQ" target="_blank">Code</a>
  &nbsp;/&nbsp;
  <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xie_JointSQ_Joint_Sparsification-Quantization_for_Distributed_Learning_CVPR_2024_paper.pdf" target="_blank">Paper</a>
</li>
               </ul>
            </td>
			  <tr height="30"><td colspan="2"><hr color="#eee"></td></tr>
<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <a href="img/iccv.png">
      <img src="img/iccv.png" alt="Publications" width="280" height="160" style="border-style: none">
    </a>
  </td>
  <td width="75%" valign="middle">
    <b>Allowing Oscillation Quantization: Overcoming Solution Space Limitations in Low Bit-Width Quantization</b>
    <p>
      Author: Weiying Xie (Advisor), Zihan Meng, Jitao Ma, Wenjin Guo, Haowei Li, Haonan Qin, Leyuan Fang, Yunsong Li.
    </p>
    <ul>
      <li>
        We propose AOQ, a novel quantization-aware training method that expands the quantized solution space via controlled oscillations.
      </li>
      <li>
        Has been accepted in the IEEE/CVF International Conference on Computer Vision (<b>ICCV 2025</b>).
      </li>
		<li>Code Repository: https://github.com/muzenc/AOQ</li>
    </ul>
  </td>
</tr>


          </tr>
          <tr height="30"><td colspan="2"><hr color="#eee"></td></tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="img/2.jpg"><img src="img/2.jpg" alt="Publications" width="280" height="160" style="border-style: none"></a>
            </td>
            <td width="75%" valign="middle">
            	<b>FedFQ: Federated Learning with Fine-Grained Quantization</b>
              <p>Author: Haowei Li, Weiying Xie (Advisor), Hangyu Ye, Jitao Ma, Shuran Ma and Yunsong Li.</p>
              <ul>
              	<li>We devise personalized compression strategies at the parameter level for each client to address the Non-IID characteristics of Federated Learning.</li>
				<li>Under review by THE THIRTY-NINTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-2025).</li>
                <li>Paper link:  https://arxiv.org/abs/2408.08977</li>
              </ul>
            </td>
          </tr>
           <tr height="30"><td colspan="2"><hr color="#eee"></td></tr>
         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="img/3.jpg"><img src="img/3.jpg" alt="Publications" width="280" height="160" style="border-style: none"></a>
            </td>
            <td width="75%" valign="middle">
            	<b>Reducing Spurious Correlation for Federated Domain Generalization</b>
              <p>Author: Shuran Ma, Weiying Xie (Advisor), Daixun Li, Haowei Li, Yunsong Li.</p>
              <ul>
              	<li>We propose FedCD, a framework tackling cross-domain generalization in federated learning with IMG and REA methods. FedCD outperforms baselines, improving accuracy by 1.45% and mAP50 by 4.36%.</li>
		
                <li>Paper link: https://arxiv.org/abs/2407.19174</li>
              </ul>
            </td>
          </tr>
          
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>PROJECT EXPERIENCE</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="img/海岸线.png"><img src="img/海岸线.png" alt="PROJECT EXPERIENCE" width="280" height="160" style="border-style: none"></a>
            </td>
            <td width="75%" valign="middle">
              <b>Coastline extraction technique based on SAM (Segment Anything Model)</b>
              <ul>
              	<li>We are using SAM in the field of multimodal fusion, and we propose a cross-domain prompt learning model.</li>
		      <li>We have designed a multi-satellite intelligent interpretation system for the distributed deployment of SAM.</li>
			      <li>The relevant work has been implemented for coastline extraction in Uzbekistan. The average accuracy obtained from remote sensing data in Uzbekistan is 91.64%.</li>
               </ul>
            </td>
          </tr>
			<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <a href="img/火星.png">
      <img src="img/火星.png" alt="PROJECT EXPERIENCE" width="280" height="160" style="border-style: none">
    </a>
  </td>
  <td width="75%" valign="middle">
    ChatMars: A Unified Multimodal Vision–Language Model for Planetary and Earth Studies
    <ul>
      <li>
        We introduce Mars-VL-227k, a unified multimodal image–text dataset designed for planetary exploration and Earth observation.
      </li>
      <li>
        We develop Chat-Mars-LLAMA3, a multimodal vision–language model that integrates tasks from both planetary science and Earth-based studies.
      </li>
      <li>
        I am responsible for the distributed training and training acceleration of the model, enabling scalable and efficient large-scale training.
      </li>
    </ul>
  </td>
</tr>


 
          
        </tbody></table>


 		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>AWARDS AND RECOGNITION</heading>
                <p>
                <ul>
                  <li style="line-height:30px">IEEE student member.</li>
					<li style="line-height:30px">Milton and Albertha Langdon Memorial Graduate Fellowship(2025-2026).</li>
                  <li style="line-height:30px">Graduate Outstanding Academic Scholarship, Xidian University (2024).</li>
                  <li style="line-height:30px">Undergraduate School-Level Scholarship, Xidian University (2022).</li>
                  <li style="line-height:30px">Third Prize in the 2nd "Fire Cup" Target Intelligent Recognition Competition (2023).</li>
                  </ul>
                </p>
            </td>
          </tr>
        </tbody></table>


 		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>WORK EXPERIENCE</heading>
                <p>
                <ul>
                  <li style="line-height:30px; list-style:none;"><b>Fibocom Wireless Inc</b>, <i>Xi’an, China</i><span style="float:right;">03/2022-07/2022</span></li>
                  <li style="line-height:30px"><b>Department</b>: Software Development (Intern)</li>
                  <li style="line-height:30px"><b>Position</b>: Project Leader of the “Facial Recognition Attendance System”</li>
                  <li style="line-height:30px">We are responsible for establishing an employee facial database and developing a facial recognition system on a demo board, based on FaceNet (an open-source project utilizing CNN). Additionally, we are 
developing an APP to provide user APIs and handle the system's daily maintenance and updates.</li>
                  </ul>
                </p>
            </td>
          </tr>
        </tbody></table>



 		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>MISC</heading>
                <p>
                <ul>
                  <li style="line-height:30px">Here are the photos I took when I attended CVPR 2024:<a href="img/4.jpg" target="_blank">[here]</a></heading><br></li>
                  <li style="line-height:30px">This is the student club I joined, and it's about pop music:<a href="img/5.jpg" target="_blank">[here]</a></heading></li>

                  </ul>
                </p>
            </td>
          </tr>
        </tbody></table>
	



 

      </td>
    </tr>
  </table>
</body>

</html>
